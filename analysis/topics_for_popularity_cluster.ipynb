{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### We generated clusters in counting_bibleverses_with_py_collections. Now lets try to find topics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# see http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
      "# http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html\n",
      "import pyes\n",
      "import json\n",
      "from datetime import datetime, timedelta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# bv_set = json.loads('[\"ephesians 2:8\", \"hebrews 13:16\", \"i_corinthians 13:4\", \"ii_chronicles 15:7\", \"isaiah 32:17\", \"james 1:19\", \"luke 6:27\", \"matthew 21:22\", \"matthew 5:8\", \"philippians 4:13\", \"proverbs 17:28\", \"revelation 1:8\"]')   \n",
      "bv_set = [\"john 15:13\"]\n",
      "# bv_set = [\"john 13:7\", \"matthew 21:22\", \"philippians 4:13\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print bv_set, len(bv_set)\n",
      "bv_tokens = []\n",
      "[bv_tokens.extend(bv.replace(\":\",\" \").split()) for bv in bv_set]\n",
      "print bv_tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['john 15:13'] 1\n",
        "['john', '15', '13']\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyes import ES\n",
      "from pyes.query import MatchAllQuery, FilteredQuery, BoolQuery, MatchQuery, TextQuery\n",
      "from pyes.filters import RangeFilter, TermFilter, QueryFilter, ANDFilter\n",
      "from pyes.utils import ESRange, ESRangeOp\n",
      "\n",
      "conn = ES('192.168.117.4:9201')\n",
      "ts_field='created_at_date'\n",
      "start = datetime.now() + timedelta(days=-15)\n",
      "start = start.strftime(\"%Y-%m-%d\")\n",
      "start"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 61,
       "text": [
        "'2014-05-15'"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# filter on days\n",
      "import re\n",
      "def get_text_from_es(bibleverse, start):\n",
      "    q = MatchAllQuery()\n",
      "    filters = []\n",
      "    filters.append(RangeFilter(qrange=ESRangeOp(field=ts_field, op1='gte',value1=start)))\n",
      "    filters.append(TermFilter(field=\"bibleverse\",value=bibleverse))\n",
      "    q = FilteredQuery(MatchAllQuery(), ANDFilter(filters))\n",
      "    q = q.search(size=20)\n",
      "    resultset = conn.search(indices=[\"habakkuk-all\"], doc_types=[\"habakkuk\"], query=q, size=20)\n",
      "    print \"Total results %s %s docs\"%(bibleverse, resultset.total)\n",
      "    res = list()\n",
      "    for r in resultset:\n",
      "        t = re.sub(\"[@#]\\w+(?:$|\\W)\",\"\", r['text'])\n",
      "        res.append(t)\n",
      "    return res\n",
      "                                 \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = []\n",
      "for bv in bv_set:\n",
      "    ret = get_text_from_es(bv, start)\n",
      "    corpus.append(\" \".join(ret))\n",
      "    print len(\" \".join(ret))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total results john 15:13 128 docs\n",
        "1959\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html\n",
      "from sklearn import decomposition\n",
      "import time\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
      "# from __future__ import print_function\n",
      "\n",
      "n_samples = len(corpus)\n",
      "n_features = 1000\n",
      "n_topics = 1 # len(corpus)\n",
      "print (\"n_topics\",n_topics)\n",
      "n_top_words = 3\n",
      "\n",
      "# vectorize the tweet text using the most common word\n",
      "# frequency with TF-IDF weighting (without the top 5% stop words)\n",
      "vectorizer = TfidfVectorizer(max_features=n_features, \n",
      "                             ngram_range=(1,3)\n",
      "                             )\n",
      "stoplist = ['retweet', 'rt', 'http']\n",
      "print bv_tokens\n",
      "vectorizer.set_params(stop_words=set(list(ENGLISH_STOP_WORDS)+stoplist+bv_tokens))\n",
      "tfidf = vectorizer.fit_transform(corpus)\n",
      "feature_names = vectorizer.get_feature_names()\n",
      "print \"num_feature_names\",len(feature_names)\n",
      "\n",
      "# Fit the NMF model\n",
      "print \"Fitting the NMF model on with n_samples=%d and n_features=%d...\"\\\n",
      "  % (n_samples, n_features)\n",
      "nmf = decomposition.NMF(n_components=n_topics, random_state=42).fit(tfidf)\n",
      "\n",
      "cluster_topics = []\n",
      "for topic_idx, topic in enumerate(nmf.components_):\n",
      "    print(\"Topic #%d:\" % topic_idx)\n",
      "    cluster_topics.append(set([feature_names[i]\n",
      "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
      "    print(\", \".join([feature_names[i]\n",
      "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
      "    print()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('n_topics', 1)\n",
        "['john', '15', '13']\n",
        "num_feature_names 103\n",
        "Fitting the NMF model on with n_samples=1 and n_features=1000...\n",
        "Topic #0:\n",
        "love, friends, greater love\n",
        "()\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    }
   ],
   "metadata": {}
  }
 ]
}